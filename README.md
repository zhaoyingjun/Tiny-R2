# Tiny-R2 A better combination: DSA/SWA/MLA, mHC, and DSMoE 

# Tiny-R2 æ¨¡å‹æ¶æ„ä¸è®­ç»ƒæµç¨‹æ–‡æ¡£


---

## ğŸ“‹ ç›®å½•

1. [é¡¹ç›®æ¦‚è¿°](#é¡¹ç›®æ¦‚è¿°)
2. [æ¨¡å‹æ¶æ„æ€»è§ˆ](#æ¨¡å‹æ¶æ„æ€»è§ˆ)
3. [æ ¸å¿ƒç»„ä»¶è¯¦è§£](#æ ¸å¿ƒç»„ä»¶è¯¦è§£)
   - 3.1 [æ³¨æ„åŠ›æœºåˆ¶](#31-æ³¨æ„åŠ›æœºåˆ¶)
   - 3.2 [MLA-NSA æ··åˆæ³¨æ„åŠ›](#32-mla-nsa-æ··åˆæ³¨æ„åŠ›)
   - 3.3 [å‰é¦ˆç½‘ç»œä¸ MoE](#33-å‰é¦ˆç½‘ç»œä¸-moe)
   - 3.4 [Hyper-Connections](#34-hyper-connections)
4. [è®­ç»ƒæµç¨‹](#è®­ç»ƒæµç¨‹)
5. [å…³é”®æŠ€æœ¯ç‰¹æ€§](#å…³é”®æŠ€æœ¯ç‰¹æ€§)
6. [é™„å½•ï¼šå›¾è¡¨ç´¢å¼•](#é™„å½•å›¾è¡¨ç´¢å¼•)

---

## é¡¹ç›®æ¦‚è¿°

Tiny-R2 æ˜¯ä¸€ä¸ªç´§å‡‘å‹ä½†åŠŸèƒ½å¼ºå¤§çš„è¯­è¨€æ¨¡å‹ï¼Œç»“åˆäº†å¤šç§å…ˆè¿›çš„æ·±åº¦å­¦ä¹ æŠ€æœ¯ï¼š

- **ç¨€ç–æ³¨æ„åŠ›æœºåˆ¶** (MLA-NSA Hybrid Attention)
- **ä¸“å®¶æ··åˆæ¨¡å‹** (DeepSeek MoE)
- **è¶…è¿æ¥æŠ€æœ¯** (Hyper-Connections)
- **åŒä¼˜åŒ–å™¨ç­–ç•¥** (Muon + AdamW)

---

## æ¨¡å‹æ¶æ„æ€»è§ˆ

```
               Input Tokens
                      â†“
Token Embedding + Positional Embedding
                      â†“
       Hyper-Connection Expand Stream
                      â”‚
                      â–¼
-------------------------------------------------------
                  RMSNorm â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”               |
                      â”‚               â”‚               |
                      â–¼               â”‚ Residual      |
              â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”       â”‚ Connection    |
              â”‚  Attention    â”‚       â”‚               |
              â”‚ (NSA/SWA/DSA) |       |               |
              |     ä¸‰é€‰ä¸€     â”‚       â”‚               |
              â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜       â”‚               |
                      â”‚               â”‚               |
                      â–¼               â”‚               |
           Hyper-connection (hc_attn) â”‚               |
                      â”‚               â”‚               |
                      â–¼               â”‚       N*Transformer Block
                  RMSNorm â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤               |
                      â”‚               â”‚               |
                      â–¼               â”‚               |
              â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”       â”‚               |
              â”‚  Dense / MoE  â”‚       â”‚               |
              â”‚  MLP/DSMoE    |       |               |
              |    äºŒé€‰ä¸€      â”‚       â”‚               |
              â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜       â”‚               |
                      â”‚               â”‚               |
                      â–¼               â”‚               |
           Hyper-connection (hc_mlp)  â”‚               |
                      â”‚               â”‚               |
                      â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜               |
                      â”‚                               |
                      â–¼                               |
            Output + router_weights                   |
                      â†“                               |
-----------------------------------------------------
         Hyper-Connection Reduce Stream
                      â†“
               RMSNorm + LM Head
                       â†“
                 Output Logits
```

---

## æ ¸å¿ƒç»„ä»¶è¯¦è§£

### 3.1 æ³¨æ„åŠ›æœºåˆ¶

Tiny-R2 æ”¯æŒä¸¤ç§æ³¨æ„åŠ›ç±»å‹ï¼Œé€šè¿‡é…ç½® `attention_types` çµæ´»åˆ‡æ¢ï¼š

#### 3.1.1 CausalSelfAttention (Full Attention)

æ ‡å‡†çš„å› æœè‡ªæ³¨æ„åŠ›æœºåˆ¶ï¼š

```python
class CausalSelfAttention(nn.Module):
    def __init__(self, config):
        # Projections: Q, K, V from single linear
        self.c_attn = nn.Linear(config.n_embd, 3 * config.n_embd)
        self.c_proj = nn.Linear(config.n_embd, config.n_embd)
        
        # Value residual connections
        self.v_residual = config.v_residual
        self.lamb1 = nn.Parameter(torch.tensor(0.5))
        self.lamb2 = nn.Parameter(torch.tensor(0.5))
        
        # Flash Attention support
        self.flash = hasattr(F, "scaled_dot_product_attention")
```

**å…³é”®ç‰¹æ€§ï¼š**
- ä½¿ç”¨ Flash Attention åŠ é€Ÿï¼ˆå¦‚æœå¯ç”¨ï¼‰
- æ”¯æŒ Value Residual Connections
- æ ‡å‡†çš„å› æœæ©ç 

#### 3.1.2 MLA-NSA Hybrid Attention

ç»“åˆ Multi-head Latent Attention (MLA) å’Œ Native Sparse Attention (NSA) çš„æ··åˆæ³¨æ„åŠ›æœºåˆ¶ã€‚

**ä¸‰ç§è¿è¡Œæ¨¡å¼ï¼š**

| æ¨¡å¼ | åˆ†æ”¯é…ç½® | è¯´æ˜ |
|------|----------|------|
| `NSA` | [1, 1, 1] | å¯ç”¨æ‰€æœ‰ä¸‰ä¸ªåˆ†æ”¯ |
| `SWA` | [1, 0, 1] | å‹ç¼©åˆ†æ”¯ + æ»‘åŠ¨çª—å£åˆ†æ”¯ |
| `DSA` | [1, 1, 0] | å‹ç¼©åˆ†æ”¯ + é€‰æ‹©åˆ†æ”¯ |

---

### 3.2 MLA-NSA æ··åˆæ³¨æ„åŠ›

MLA-NSA æ˜¯ Tiny-R2 çš„æ ¸å¿ƒåˆ›æ–°ä¹‹ä¸€ï¼Œé€šè¿‡ä¸‰ä¸ªå¹¶è¡Œåˆ†æ”¯å®ç°é«˜æ•ˆçš„ç¨€ç–æ³¨æ„åŠ›è®¡ç®—ã€‚

#### æ¶æ„æµç¨‹

```
                       Input x
                          â†“
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ Query Preparation (MLA style)                                â”‚
â”‚   compress_q â†’ q_norm â†’ decompress_q â†’ RoPE â†’ Query         â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                          â†“
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚   Branch 1       â”‚   Branch 2       â”‚   Branch 3           â”‚
â”‚   Compression    â”‚   Selection      â”‚   Sliding Window     â”‚
â”‚   (MLA)          â”‚   (DSA)          â”‚   (SWA)              â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ compress_kv      â”‚ importance_score â”‚ window_k/v           â”‚
â”‚ kv_norm          â”‚ topk selection   â”‚ sliding_window       â”‚
â”‚ decompress_k/v   â”‚ selection_k/v    â”‚ RoPE                 â”‚
â”‚ k_rope           â”‚ RoPE             â”‚                      â”‚
â”‚ K/V Recombine    â”‚ K/V Selected     â”‚ K/V Window           â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
    â†“                    â†“                    â†“
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ Attention Computation                                        â”‚
â”‚   Attention 1: (Q @ K1.T) @ V1                               â”‚
â”‚   Attention 2: (Q @ K2.T) @ V2                               â”‚
â”‚   Attention 3: (Q @ K3.T) @ V3                               â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
    â†“
branch_gate (Linear + Softmax) â†’ Weighted Sum
    â†“
proj (Linear) â†’ res_dropout â†’ Output
```

#### å…³é”®å‚æ•°

```python
# MLA å‚æ•°
self.v_head_dim = 32
self.kv_lora_rank = 32
self.q_lora_rank = 3 * self.kv_lora_rank
self.rope_head_dim = 64
self.nope_head_dim = 32

# NSA å‚æ•°
self.block_size = config.block_size      # Tokenå‹ç¼©å—å¤§å°
self.window_size = config.window_size    # æ»‘åŠ¨çª—å£å¤§å°
self.num_tokens_to_keep = config.num_tokens_to_keep  # é€‰æ‹©ä¿ç•™çš„tokenæ•°
```

---

### 3.3 å‰é¦ˆç½‘ç»œä¸ MoE

#### 3.3.1 MLP

æ ‡å‡†çš„å‰é¦ˆç½‘ç»œï¼Œä½¿ç”¨ ReLUÂ² æ¿€æ´»å‡½æ•°ï¼š

```python
class MLP(nn.Module):
    def __init__(self):
        self.c_fc = nn.Linear(n_embd, 4 * n_embd)
        self.c_proj = nn.Linear(4 * n_embd, n_embd)
    
    def forward(self, x):
        x = self.c_fc(x)
        x = F.relu(x).square()  # ReLU squared
        x = self.c_proj(x)
        return x
```

#### 3.3.2 DSMoE (DeepSeek Mixture of Experts)

DeepSeek é£æ ¼çš„ä¸“å®¶æ··åˆæ¨¡å‹ï¼š

```
Input x [B, T, C]
    â†“
Gate Network (Linear + UnitCenteredNoise)
    â†“
Softmax â†’ Top-k Selection
    â†“
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                    Expert Networks                          â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â” â”‚
â”‚  â”‚ Shared Exp 0 â”‚  â”‚ Expert 1 â”‚  â”‚ Expert 2 â”‚  â”‚  ...   â”‚ â”‚
â”‚  â”‚ (Always On)  â”‚  â”‚ (Top-k)  â”‚  â”‚ (Top-k)  â”‚  â”‚ (Top-k)â”‚ â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”€â”€â”€â”€â”˜ â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
    â†“
Weighted Sum of Expert Outputs
    â†“
Output [B, T, C]
```

**å…³é”®ç‰¹æ€§ï¼š**

| ç‰¹æ€§ | è¯´æ˜ |
|------|------|
| Shared Expert | å§‹ç»ˆæ¿€æ´»çš„å…±äº«ä¸“å®¶ï¼Œæä¾›ç¨³å®šæ€§ |
| Routed Experts | Top-k é€‰æ‹©çš„è·¯ç”±ä¸“å®¶ |
| Load Balance Loss | é˜²æ­¢ä¸“å®¶å´©æºƒçš„è´Ÿè½½å‡è¡¡æŸå¤± |
| Expert Bias | å¯å­¦ä¹ çš„ä¸“å®¶åç½®ï¼Œç”¨äºè·¯ç”±ä¼˜åŒ– |
| UnitCenteredNoise | è®­ç»ƒæ—¶æ·»åŠ å™ªå£°ä»¥å¢åŠ æ¢ç´¢ |

**Load Balance Loss è®¡ç®—ï¼š**

```python
def moe_load_balance_loss(router_weights, num_experts):
    load = router_weights.sum(dim=0)
    load = load / load.sum()
    ideal = torch.full_like(load, 1.0 / num_experts)
    loss = num_experts * torch.sum((load - ideal) ** 2)
    return loss
```

---

### 3.4 Hyper-Connections

Hyper-Connections æ˜¯ Tiny-R2 çš„å¦ä¸€å¤§åˆ›æ–°ï¼Œé€šè¿‡å¤šæµè·¯ç”±æœºåˆ¶å¢å¼ºä¿¡æ¯æµåŠ¨ã€‚

**æ ¸å¿ƒæ¦‚å¿µï¼š**

```python
# åˆå§‹åŒ– Hyper-Connections
self.init_hc, self.expand_stream, self.reduce_stream = \
    get_init_and_expand_reduce_stream_functions(
        config.hc_num_streams,
        num_fracs=config.hc_num_fracs,
        disable=config.hc_disable,
    )

# åœ¨æ¯ä¸ª Block ä¸­ä½¿ç”¨
self.hc_attn = init_hc(
    dim=config.n_embd,
    branch=self.attn_branch,
    layer_index=index * 2,
    mhc=config.mhc,
    sinkhorn_iters=config.sinkhorn_iters,
    sinkhorn_tau=config.sinkhorn_tau,
)
```

**å…³é”®å‚æ•°ï¼š**

| å‚æ•° | è¯´æ˜ |
|------|------|
| `hc_num_streams` | è¶…è¿æ¥æµæ•°é‡ |
| `hc_num_fracs` | åˆ†æ®µæ•°é‡ |
| `mhc` | å¤šè¶…è¿æ¥é…ç½® |
| `sinkhorn_iters` | Sinkhorn ç®—æ³•è¿­ä»£æ¬¡æ•° |
| `sinkhorn_tau` | Sinkhorn æ¸©åº¦å‚æ•° |

---

## è®­ç»ƒæµç¨‹

### 4.1 åˆå§‹åŒ–é˜¶æ®µ

```
Parse Arguments â†’ Update Config â†’ Init WandB â†’ Setup Distributed â†’ Setup AMP
```

### 4.2 æ•°æ®å‡†å¤‡

```
Load HF Dataset (flytech/python-codes-25k)
    â†“
Init GPT2 Tokenizer
    â†“
Create TokenBuffer
```

**TokenBuffer åŠŸèƒ½ï¼š**
- æµå¼è¯»å– HuggingFace æ•°æ®é›†
- åŠ¨æ€å¡«å…… token buffer
- ç”Ÿæˆè¿ç»­çš„ token batch

### 4.3 æ¨¡å‹åˆå§‹åŒ–

```
Create Transformer
    â†“
Configure Optimizers (Muon + AdamW)
    â†“
Create LR Scheduler (Warmup + Cosine)
```

### 4.4 è®­ç»ƒå¾ªç¯

```
For iter in range(max_iters):
    â”‚
    â”œâ”€â”€ For step in grad_accum_steps:
    â”‚       â”œâ”€â”€ Get Batch (TokenBuffer)
    â”‚       â”œâ”€â”€ Forward Pass (model)
    â”‚       â”œâ”€â”€ Backward Pass (scaler.scale)
    â”‚       â””â”€â”€ Collect Router Weights
    â”‚
    â”œâ”€â”€ Gradient Clipping (clip_grad_norm_)
    â”œâ”€â”€ Optimizer Steps (Muon + AdamW)
    â”œâ”€â”€ Update Scaler (scaler.update)
    â”œâ”€â”€ LR Scheduler Step
    â”œâ”€â”€ Update Expert Biases (load balancing)
    â””â”€â”€ Log Metrics (WandB)
```

### 4.5 è¯„ä¼°ä¸ä¿å­˜

```
If iter % eval_interval == 0:
    â”œâ”€â”€ Estimate Loss (eval mode)
    â”œâ”€â”€ Save Checkpoint (if val_loss < 5.27)
    â””â”€â”€ Log to WandB
```

### 4.6 ä¼˜åŒ–å™¨é…ç½®

Tiny-R2 ä½¿ç”¨åŒä¼˜åŒ–å™¨ç­–ç•¥ï¼š

```python
def configure_optimizers(self, weight_decay, learning_rate, device):
    muon_params = []    # â‰¥2D parameters in blocks
    adamw_params = []   # Other parameters
    
    for name, param in self.named_parameters():
        if 'blocks' in name and param.ndim >= 2:
            muon_params.append(param)
        else:
            adamw_params.append(param)
    
    return [
        Muon(muon_params, lr=0.02, momentum=0.95),
        torch.optim.AdamW(adamw_params, lr=learning_rate, 
                          betas=(0.90, 0.95), weight_decay=weight_decay)
    ]
```

---

## å…³é”®æŠ€æœ¯ç‰¹æ€§

### 5.1 æ³¨æ„åŠ›æœºåˆ¶å¯¹æ¯”

| ç‰¹æ€§ | CausalSelfAttention | MLA-NSA Hybrid |
|------|---------------------|----------------|
| è®¡ç®—å¤æ‚åº¦ | O(nÂ²) | O(n) ~ O(n log n) |
| å†…å­˜ä½¿ç”¨ | é«˜ | ä½ |
| é€‚ç”¨åœºæ™¯ | çŸ­åºåˆ— | é•¿åºåˆ— |
| åˆ†æ”¯æ•°é‡ | 1 | 3 (å¯é…ç½®) |

### 5.2 FFN ç±»å‹å¯¹æ¯”

| ç‰¹æ€§ | MLP | DSMoE |
|------|-----|-------|
| å‚æ•°é‡ | å›ºå®š | å…±äº« + è·¯ç”± |
| è®¡ç®—é‡ | å›ºå®š | ç¨€ç–æ¿€æ´» |
| è¡¨è¾¾èƒ½åŠ› | æ ‡å‡† | æ›´å¼º |
| è®­ç»ƒç¨³å®šæ€§ | é«˜ | éœ€è¦è´Ÿè½½å‡è¡¡ |

### 5.3 æ ¸å¿ƒé…ç½®å‚æ•°

```python
# æ¨¡å‹æ¶æ„
n_embd = 512        # åµŒå…¥ç»´åº¦
n_head = 8          # æ³¨æ„åŠ›å¤´æ•°
n_layer = 8         # å±‚æ•°
n_experts = 8       # ä¸“å®¶æ•°é‡
num_exp = 2         # æ¯tokenæ¿€æ´»çš„ä¸“å®¶æ•°

# æ³¨æ„åŠ›é…ç½®
attention_types = ["FULL", "Spares", ...]  # æ¯å±‚æ³¨æ„åŠ›ç±»å‹
attention_mode = ["FULL", "SWA", "NSA"]    # ç¨€ç–æ³¨æ„åŠ›æ¨¡å¼

# Hyper-Connections
hc = True           # å¯ç”¨è¶…è¿æ¥
hc_num_streams = 4  # æµæ•°é‡

# è®­ç»ƒ
batch_size = 32
ctx_len = 512       # ä¸Šä¸‹æ–‡é•¿åº¦
lr = 1e-3
warmup_iters = 1000
max_iters = 100000
```

---

## é™„å½•ï¼šå›¾è¡¨ç´¢å¼•

æœ¬æ–‡æ¡£é…å¥—å›¾è¡¨ä¿å­˜åœ¨ `/mnt/okcomputer/output/` ç›®å½•ï¼š

| æ–‡ä»¶å | è¯´æ˜ |
|--------|------|
| `model_architecture.png` | æ¨¡å‹æ•´ä½“æ¶æ„å›¾ |
| `mla_nsa_attention.png` | MLA-NSA æ··åˆæ³¨æ„åŠ›è¯¦ç»†ç»“æ„å›¾ |
| `dsmoe_architecture.png` | DSMoE ä¸“å®¶æ··åˆç»“æ„å›¾ |
| `training_pipeline.png` | å®Œæ•´è®­ç»ƒæµç¨‹å›¾ |
| `tinyr2_overview.png` | Tiny-R2 ç»¼åˆæ¦‚è§ˆå›¾ |

---

## å‚è€ƒèµ„æ–™

- [Tiny-R2 GitHub Repository](https://github.com/zhaoyingjun/Tiny-R2)
- [DeepSeek-V2 Technical Report](https://arxiv.org/abs/2405.04434)
- [Native Sparse Attention (NSA)](https://arxiv.org/abs/2502.04543)
- [Hyper-Connections Paper](https://arxiv.org/abs/2409.19607)

---

*æ–‡æ¡£ç”Ÿæˆæ—¶é—´: 2026-02-16*
